{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EP #4 - Object Recognition: BoF vs ConvNets\n",
    "\n",
    "Renato Sérgio Lopes Júnior \\\n",
    "2020667570"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import sklearn\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('tableau-colorblind10')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "\n",
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-1.0, -1.0, -1.0],\n",
    "    std=[1/0.5, 1/0.5, 1/0.5]\n",
    ")\n",
    "\n",
    "def get_cv2_image(img):\n",
    "    return cv2.cvtColor((inv_normalize(img).numpy().transpose(1, 2, 0) * 255).astype(dtype=np.uint8), cv2.COLOR_RGB2BGR)\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,2))\n",
    "for i in range(1, 6):\n",
    "    X, y = trainset[np.random.randint(len(trainset))]\n",
    "    img = get_cv2_image(X)\n",
    "    plt.subplot(1, 5, i)\n",
    "    plt.title(classes[y])\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classifier based on bag of features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting SIFT features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.SIFT_create(nfeatures=100)\n",
    "\n",
    "def get_sift_features(dataset):\n",
    "    sift_features = {i: [] for i in range(NUM_CLASSES)}\n",
    "\n",
    "    # Get SIFT descriptions for all samples in dataset\n",
    "    for i in range(len(dataset)):\n",
    "        if ((i+1)/len(dataset)*100)%10 == 0:\n",
    "            print(f\"{(i+1)/len(dataset)*100}%\")\n",
    "        X, y = dataset[i]\n",
    "        img = get_cv2_image(X)\n",
    "        kp, des = sift.detectAndCompute(img, None)\n",
    "        if des is None:\n",
    "            continue\n",
    "        sift_features[y].append(des)\n",
    "    \n",
    "    return sift_features\n",
    "\n",
    "def get_descriptions_matrix(sift_features):\n",
    "    # Returns matrix with SIFT descriptions (for clustering)\n",
    "    descriptions = []\n",
    "    for i in range(NUM_CLASSES):\n",
    "        for j in range(len(sift_features[i])):\n",
    "            descriptions.extend(sift_features[i][j])\n",
    "    descriptions = np.array(descriptions)\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SIFT descriptions for training samples\n",
    "train_sift_features = get_sift_features(trainset)\n",
    "train_descriptions = get_descriptions_matrix(train_sift_features)\n",
    "\n",
    "# Get SIFT descriptions for test samples\n",
    "test_sift_features = get_sift_features(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create visual dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_clustering(descriptions, dict_size):\n",
    "    # Cluster descriptions\n",
    "    clustering = MiniBatchKMeans(n_clusters=dict_size).fit(descriptions)\n",
    "    return clustering\n",
    "\n",
    "def get_visual_dict(clustering, descriptions, dict_size):\n",
    "    hist = np.zeros(dict_size)\n",
    "    for desc in descriptions:\n",
    "        word = clustering.predict(desc.reshape(1, -1))\n",
    "        hist[word[0]] += 1\n",
    "    cv2.normalize(hist, hist, norm_type=cv2.NORM_L2)\n",
    "    return hist\n",
    "\n",
    "def get_bof_data(clustering, sift_features, dict_size):\n",
    "    X = []\n",
    "    y = []\n",
    "    # Get visual dictionary for each image\n",
    "    for i in range(NUM_CLASSES):\n",
    "        if ((i+1)/NUM_CLASSES*100)%10 == 0:\n",
    "            print(f\"{(i+1)/NUM_CLASSES*100}%\")\n",
    "        for j in range(len(sift_features[i])):\n",
    "            bof = get_visual_dict(clustering, sift_features[i][j], dict_size)\n",
    "            X.append(bof)\n",
    "            y.append(i)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest Classifier with Dictionary Size 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create clustering\n",
    "print(\"Creating cluster...\")\n",
    "clustering = create_clustering(train_descriptions, dict_size=8)\n",
    "print(\"Created cluster.\")\n",
    "\n",
    "# Get training data visual dictionaries\n",
    "print(\"Getting visual dictionaries...\")\n",
    "bof_X, bof_y = get_bof_data(clustering, train_sift_features, dict_size=8)\n",
    "print(\"Got visual dictionaries.\")\n",
    "\n",
    "# Get visual dictionaries for test samples\n",
    "print(\"Getting visual dictionaries...\")\n",
    "bof_X_test, bof_y_test = get_bof_data(clustering, test_sift_features, dict_size=8)\n",
    "print(\"Got visual dictionaries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create Random Forest Classifier\n",
    "forest_classifier = RandomForestClassifier()\n",
    "# Fit with visual dictionaries\n",
    "forest_classifier.fit(bof_X, bof_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Metrics\n",
    "sklearn.metrics.plot_confusion_matrix(forest_classifier, bof_X_test, bof_y_test)\n",
    "predictions = forest_classifier.predict(bof_X_test)\n",
    "print(\"Accuracy\", sklearn.metrics.accuracy_score(bof_y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A classifier using ConvNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSIZE = 128\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCHSIZE, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCHSIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.feature = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(6, 16, kernel_size=5, stride=1),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(400, 120),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(84, NUM_CLASSES),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data):\n",
    "        feature = self.feature(data)\n",
    "        return self.classifier(feature)\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu')\n",
    "net = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 50\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9, weight_decay=0.005)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # Training\n",
    "    net.train()\n",
    "    loss_epoch_train = 0.0\n",
    "    corrects_epoch_train = 0\n",
    "    for X, y in trainloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        out = net(X)\n",
    "        \n",
    "        loss = loss_fn(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        prob, pred = torch.max(out.data, 1)\n",
    "        corrects_epoch_train += (pred == y.long()).sum().item()\n",
    "        loss_epoch_train += loss.item()\n",
    "    \n",
    "    # Evaluation\n",
    "    net.eval()\n",
    "    loss_epoch_eval = 0.0\n",
    "    corrects_epoch_eval = 0\n",
    "    for X, y in testloader:\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = net(X)\n",
    "            loss = loss_fn(out, y)\n",
    "        \n",
    "        prob, pred = torch.max(out.data, 1)\n",
    "        corrects_epoch_eval += (pred == y.long()).sum().item()\n",
    "        loss_epoch_eval += loss.item()\n",
    "    \n",
    "    loss_epoch_train /= len(trainloader)\n",
    "    loss_epoch_eval /= len(testloader)\n",
    "    acc_epoch_train = corrects_epoch_train/len(trainloader.dataset)\n",
    "    acc_epoch_eval = corrects_epoch_eval/len(testloader.dataset)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: train_loss={loss_epoch_train:.4f}, train_acc={acc_epoch_train:.4f}, test_loss={loss_epoch_eval:.4f}, test_acc={acc_epoch_eval:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
